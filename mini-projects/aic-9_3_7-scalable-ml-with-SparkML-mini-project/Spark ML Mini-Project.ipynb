{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Existence of Spark Environment Variables\n",
    "\n",
    "Make sure your notebook is loaded using a PySpark Workspace. If you open up a regular Jupyter workspace the following variables might not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the following if you failed to open a notebook in the PySpark Workspace\n",
    "\n",
    "This will work assuming you are using Spark in the cloud on domino or you might need to configure with your own spark instance if you are working offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 'sc' not in locals():\n",
    "    from pyspark.context import SparkContext\n",
    "    from pyspark.sql.context import SQLContext\n",
    "    from pyspark.sql.session import SparkSession\n",
    "    \n",
    "    sc = SparkContext()\n",
    "    sqlContext = SQLContext(sc)\n",
    "    spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a utility function to run SQL commands\n",
    "\n",
    "Instead of typing the same python functions repeatedly, we build a small function where you can just pass your query to get results.\n",
    "\n",
    "- Remember we are using Spark SQL in PySpark\n",
    "- We can't run multiple SQL statements in one go (no semi-colon ';' separated SQL statements)\n",
    "- We can run multi-line SQL queries (but still has to be a single statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_sql(statement):\n",
    "    try:\n",
    "        result = sqlContext.sql(statement)\n",
    "    except Exception as e:\n",
    "        print(e.desc, '\\n', e.stackTrace)\n",
    "        return\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbls = run_sql('show tables')\n",
    "tbls.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sql('drop table if exists adult')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Dataset\n",
    "\n",
    "Below we will use Spark SQL to load in the data and then register it as a Dataframe. So the end result will be a Spark SQL table called `adult` and a Spark Dataframe called `adult_df`. \n",
    "\n",
    "This is an example of the flexibility in Spark in that you could do lots of you ETL and data wrangling using either Spark SQL or Dataframes and pyspark. Most of the time it's a case of using whatever you are most comfortable with. \n",
    "\n",
    "When you get more advanced then you might looking the pro's and con's of each and when you might favour one or the other (or operating direclty on RDD's), [here](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html) is a good article on the issues. For now, no need to overthink it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the DataFrame\n",
    "\n",
    "In this section, we will be creating a spark dataframe from the `adult` dataset which is easier work with when building machine learning models.\n",
    "\n",
    "To get started, first make sure you have already uploaded the `adult.data.csv` CSV file and it is present in the same directory as the notebook.\n",
    "\n",
    "Once you have done this, please remember to execute the following code to build the dataframe which can also be accessed as a table using spark SQL which we will see shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType\n",
    "\n",
    "# File location and type\n",
    "file_location_adult = \"./adult.data.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"false\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# define the schema based on the dataset dictionary\n",
    "# this is available here: https://archive.ics.uci.edu/ml/datasets/adult\n",
    "schema = StructType([\n",
    "      StructField('age', DoubleType()),\n",
    "      StructField('workclass', StringType()),\n",
    "      StructField('fnlwgt', DoubleType()),\n",
    "      StructField('education', StringType()),\n",
    "      StructField('education_num', DoubleType()),\n",
    "      StructField('marital_status', StringType()),\n",
    "      StructField('occupation', StringType()),\n",
    "      StructField('relationship', StringType()),\n",
    "      StructField('race', StringType()),\n",
    "      StructField('sex', StringType()),\n",
    "      StructField('capital_gain', DoubleType()),\n",
    "      StructField('capital_loss', DoubleType()),\n",
    "      StructField('hours_per_week', DoubleType()),\n",
    "      StructField('native_country', StringType()),\n",
    "      StructField('income', StringType())\n",
    "])\n",
    "\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "adult_df = (spark.read.format(file_type) \n",
    "                    .schema(schema)\n",
    "                    .option(\"inferSchema\", infer_schema) \n",
    "                    .option(\"header\", first_row_is_header) \n",
    "                    .option(\"sep\", delimiter) \n",
    "                    .load(file_location_adult))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing the dataframe schemas\n",
    "\n",
    "We can take a look at the schemas of our potential dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Adult Dataset Schema')\n",
    "adult_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_df.registerTempTable(\"adult\")\n",
    "tbls = run_sql('show tables')\n",
    "tbls.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = run_sql(\"SELECT * FROM adult LIMIT 5\")\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are more comfortable with SQL then as you can see below, its very easy to just get going with writing standard SQL type code to analyse your data, do data wrangling and create new dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_sql(\n",
    "  \"\"\"\n",
    "  SELECT \n",
    "    occupation,\n",
    "    SUM(1) as n,\n",
    "    ROUND(AVG(if(LTRIM(marital_status) LIKE 'Married-%',1,0)),2) as married_rate,\n",
    "    ROUND(AVG(if(lower(marital_status) LIKE '%widow%',1,0)),2) as widow_rate,\n",
    "    ROUND(AVG(if(LTRIM(marital_status) = 'Divorced',1,0)),2) as divorce_rate,\n",
    "    ROUND(AVG(if(LTRIM(marital_status) = 'Separated',1,0)),2) as separated_rate,\n",
    "    ROUND(AVG(if(LTRIM(marital_status) = 'Never-married',1,0)),2) as bachelor_rate\n",
    "  FROM \n",
    "    adult \n",
    "  GROUP BY 1\n",
    "  ORDER BY n DESC\n",
    "  \"\"\")\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easily register dataframes as a table for Spark SQL too. So this way you can easily move between Dataframes and Spark SQL as needed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register the df we just made as a table for spark sql\n",
    "sqlContext.registerDataFrameAsTable(result, \"result\")\n",
    "tbls = run_sql('show tables')\n",
    "tbls.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sql(\"SELECT * FROM result\").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn: Q1\n",
    "### Write some Spark SQL to get the top 'bachelor_rate' by 'education' group?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q1 Answer ###\n",
    "\n",
    "result = run_sql(\n",
    "  \"\"\"\n",
    "  \n",
    "  \"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark DataFrames\n",
    "Below we will create our DataFrame from the SQL table and do some similar analysis as we did with Spark SQL but using the DataFrames API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register a df from the sql df\n",
    "adult_df = spark.table(\"adult\")\n",
    "cols = adult_df.columns # this will be used much later in the notebook, ignore for now\n",
    "print(type(adult_df))\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at df schema\n",
    "adult_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful with `toPandas()` since it dumps the entire spark dataframe (which might be distributed) in your local system which might end up crashing your notebook if your resources can't handle it. A good option is always to use `limit()` to limit the rows and then take a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the DataFrame API for computations\n",
    "\n",
    "By now, you have already learnt how to leverage the Spark DataFrame API to wrangle, aggregate, filter and transform data. Following is a depiction on our dataset showcasing the divorced date based on occupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import what we will need\n",
    "from pyspark.sql.functions import when, col, mean, desc, round\n",
    "\n",
    "# wrangle the data a bit\n",
    "df_result = adult_df.select(\n",
    "  adult_df['occupation'], \n",
    "  # create a 1/0 type col on the fly\n",
    "  when( col('marital_status') == ' Divorced' , 1 ).otherwise(0).alias('is_divorced')\n",
    ")\n",
    "# do grouping (and a round)\n",
    "df_result = df_result.groupBy('occupation').agg(round(mean('is_divorced'),2).alias('divorced_rate'))\n",
    "# do ordering\n",
    "df_result = df_result.orderBy(desc('divorced_rate'))\n",
    "# show results\n",
    "df_result.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the dataframes api is a bit more verbose then just expressing what you want to do in standard SQL.\n",
    "\n",
    "But some prefer it and might be more used to it, and there could be cases where expressing what you need to do might just be better using the DataFrame API if it is too complicated for a simple SQL expression for example of maybe involves recursion of some type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn: Q2\n",
    "### Write some pyspark to get the top 'bachelor_rate' by 'education' group using DataFrame operations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q2 Answer ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore & Visualize Data\n",
    "It's very easy to convert your Spark DataFrame into a Pandas dataframe and then continue to analyse or plot as you might normally.  We have depicted the same in several example above. You can also use the `collect()` function if needed instead of `toPandas()` but you would need to call a `pandas.DataFrame` on the result.\n",
    "\n",
    "Obviously if you try to build a huge pandas DataFrame then you will run into issues, so usually the best practice is to only collect aggregated or sampled data into a Pandas df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# do some analysis\n",
    "result = run_sql(\n",
    "  \"\"\"\n",
    "  SELECT \n",
    "    occupation,\n",
    "    AVG(IF(income = ' >50K',1,0)) as plus_50k\n",
    "  FROM \n",
    "    adult \n",
    "  GROUP BY 1\n",
    "  ORDER BY 2 DESC\n",
    "  \"\"\")\n",
    "\n",
    "# collect results into a pandas df\n",
    "df_pandas = pd.DataFrame(\n",
    "  result.collect(),\n",
    "  columns=result.schema.names\n",
    ")\n",
    "\n",
    "df_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or just use a one-liner for a pandas df\n",
    "df_pandas = result.toPandas()\n",
    "df_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will just do some very basic plotting to show how you might collect what you are interested in into a Pandas DF and then just plot any way you normally would.\n",
    "\n",
    "For simplicity we are going to use the plotting functionality built into pandas (you could make this as pretty as you want). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# feel free to chose your own style!\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# get simple plot on the pandas data\n",
    "df_pandas.plot(kind='barh', x='occupation', y='plus_50k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also easily get summary stats on a Spark DataFrame like below. [Here](https://databricks.com/blog/2015/06/02/statistical-and-mathematical-functions-with-dataframes-in-spark.html) is a nice blog post that has more examples.\n",
    "\n",
    "So this is an example of when you might want to move from Spark SQL into using DataFrames API as being able to just call `describe()` on the Spark DF is easier then trying to do the equivalent in Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe df\n",
    "adult_df.select(adult_df['age'], adult_df['education_num']).describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Pipeline - Logistic Regression vs Random Forest\n",
    "\n",
    "Below we will create two Spark ML Pipelines - one that fits a logistic regression and one that fits a random forest. We will then compare the performance of each.\n",
    "\n",
    "We will be using Spark MLlib. Here are some interesting facts about the same and you can read up more in the [official docs](https://spark.apache.org/docs/latest/ml-guide.html)\n",
    "\n",
    "#### What is MLlib?\n",
    "MLlib is Spark’s machine learning (ML) library. Its goal is to make practical machine learning scalable and easy. At a high level, it provides tools such as:\n",
    "\n",
    "- ML Algorithms: common learning algorithms such as classification, regression, clustering, and collaborative filtering\n",
    "- Featurization: feature extraction, transformation, dimensionality reduction, and selection\n",
    "- Pipelines: tools for constructing, evaluating, and tuning ML Pipelines\n",
    "- Persistence: saving and load algorithms, models, and Pipelines\n",
    "- Utilities: linear algebra, statistics, data handling, etc.\n",
    "\n",
    "Some important points to remember.\n",
    "\n",
    "#### The MLlib RDD-based API is now in maintenance mode.\n",
    "As of Spark 2.0, the RDD-based APIs in the `spark.mllib` package have entered maintenance mode. The primary Machine Learning API for Spark is now the DataFrame-based API in the `spark.ml` package.\n",
    "\n",
    "#### Why is MLlib switching to the DataFrame-based API?\n",
    "DataFrames provide a more user-friendly API than RDDs. The many benefits of DataFrames include Spark Datasources, SQL/DataFrame queries, Tungsten and Catalyst optimizations, and uniform APIs across languages.\n",
    "The DataFrame-based API for MLlib provides a uniform API across ML algorithms and across multiple languages.\n",
    "DataFrames facilitate practical ML Pipelines, particularly feature transformations. See the Pipelines guide for details.\n",
    "\n",
    "#### What is “Spark ML”?\n",
    "“Spark ML” is not an official name but occasionally used to refer to the MLlib DataFrame-based API. This is majorly due to the org.apache.spark.ml Scala package name used by the DataFrame-based API, and the “Spark ML Pipelines” term we used initially to emphasize the pipeline concept.\n",
    "\n",
    "#### Is MLlib deprecated?\n",
    "No. MLlib includes both the RDD-based API and the DataFrame-based API. The RDD-based API is now in maintenance mode. But neither API is deprecated, nor MLlib as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "\n",
    "categoricalColumns = [\"workclass\", \"education\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native_country\"]\n",
    "stages = [] # stages in our Pipeline\n",
    "\n",
    "for categoricalCol in categoricalColumns:\n",
    "    # Category Indexing with StringIndexer\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
    "    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "    # encoder = OneHotEncoderEstimator(inputCol=categoricalCol + \"Index\", outputCol=categoricalCol + \"classVec\")\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    # Add stages.  These are not run here, but will run all at once later on.\n",
    "    stages += [stringIndexer, encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert label into label indices using the StringIndexer\n",
    "label_stringIdx = StringIndexer(inputCol=\"income\", outputCol=\"label\")\n",
    "stages += [label_stringIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform all features into a vector using VectorAssembler\n",
    "numericCols = [\"age\", \"fnlwgt\", \"education_num\", \"capital_gain\", \"capital_loss\", \"hours_per_week\"]\n",
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pipeline.\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "# Run the feature transformations.\n",
    "#  - fit() computes feature statistics as needed.\n",
    "#  - transform() actually transforms the features.\n",
    "pipelineModel = pipeline.fit(adult_df)\n",
    "dataset = pipelineModel.transform(adult_df)\n",
    "\n",
    "# Keep relevant columns\n",
    "selectedcols = [\"label\", \"features\"] + cols\n",
    "dataset = dataset.select(selectedcols)\n",
    "dataset.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Randomly split data into training and test sets. set seed for reproducibility\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed=100)\n",
    "print(trainingData.count())\n",
    "print(testData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# get the rate of the positive outcome from the training data to use as a threshold in the model\n",
    "training_data_positive_rate = trainingData.select(avg(trainingData['label'])).collect()[0][0] \n",
    "\n",
    "print(\"Positive rate in the training data is {}\".format(training_data_positive_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create initial LogisticRegression model\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
    "\n",
    "# set threshold for the probability above which to predict a 1\n",
    "lr.setThreshold(training_data_positive_rate)\n",
    "# lr.setThreshold(0.5) # could use this if knew you had balanced data\n",
    "\n",
    "# Train model with Training Data\n",
    "lrModel = lr.fit(trainingData)\n",
    "\n",
    "# get training summary used for eval metrics and other params\n",
    "lrTrainingSummary = lrModel.summary\n",
    "\n",
    "# Find the best model threshold if you would like to use that instead of the empirical positve rate\n",
    "fMeasure = lrTrainingSummary.fMeasureByThreshold\n",
    "maxFMeasure = fMeasure.groupBy().max('F-Measure').select('max(F-Measure)').head()\n",
    "lrBestThreshold = (fMeasure.where(fMeasure['F-Measure'] == maxFMeasure['max(F-Measure)'])\n",
    "                           .select('threshold').head()['threshold'])\n",
    "  \n",
    "print(\"Best threshold based on model performance on training data is {}\".format(lrBestThreshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check current model prediction threshold\n",
    "lrModel._java_obj.getThreshold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuild model with optimal threshold from training\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
    "\n",
    "# set threshold for the probability above which to predict a 1\n",
    "lr.setThreshold(lrBestThreshold)\n",
    "# lr.setThreshold(0.5) # could use this if knew you had balanced data\n",
    "\n",
    "# Train model with Training Data\n",
    "lrModel = lr.fit(trainingData)\n",
    "\n",
    "lrModel._java_obj.getThreshold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on test data\n",
    "lrPredictions = lrModel.transform(testData)\n",
    "\n",
    "# display predictions\n",
    "lrPredictions.select(\"label\", \"prediction\", \"probability\").limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Machines (GBM) - Training\n",
    "\n",
    "Here you will train your own GBM Classifier and make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn: Q3\n",
    "### Train a GBTClassifier on the training data, call the trained model 'gbModel'\n",
    "- Note: You don't need to set a prediction threshold here, just try training a vanilla GBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn: Q4\n",
    "### Get predictions on the test data for your GBTClassifier. Call the predictions df 'gbPredictions' and display the first 10 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on test data\n",
    "\n",
    "\n",
    "# display predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance Evaluation\n",
    "\n",
    "Without evaluation of how our models perform, there is no way for us to find out which model might be better or more suitable to solving a specific problem. Let's spend some time evaluating our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn: Q5 - Logistic Regression - Evaluation\n",
    "\n",
    "### Complete the `print_performance_metrics()` function below to also include measures of F1, Precision, Recall, False Positive Rate and True Positive Rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics\n",
    "\n",
    "def print_performance_metrics(predictions):\n",
    "    # Evaluate model\n",
    "    evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "    auc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "    aupr = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderPR\"})\n",
    "    print(\"auc = {}\".format(auc))\n",
    "    print(\"aupr = {}\".format(aupr))\n",
    "\n",
    "    # get rdd of predictions and labels for mllib eval metrics\n",
    "    predictionAndLabels = predictions.select(\"prediction\",\"label\").rdd\n",
    "\n",
    "    # Instantiate metrics objects\n",
    "    binary_metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "    multi_metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "    # Area under precision-recall curve\n",
    "    print(\"Area under PR = {}\".format(binary_metrics.areaUnderPR))\n",
    "    # Area under ROC curve\n",
    "    print(\"Area under ROC = {}\".format(binary_metrics.areaUnderROC))\n",
    "    # Accuracy\n",
    "    print(\"Accuracy = {}\".format(multi_metrics.accuracy))\n",
    "    # Confusion Matrix\n",
    "    print(multi_metrics.confusionMatrix())\n",
    "\n",
    "    ### Question 5.1 Answer ###\n",
    "\n",
    "    # F1\n",
    "    print(\"F1 = {}\".format('Add F1 Metric function here'))\n",
    "    # Precision\n",
    "    print(\"Precision = {}\".format('Add Precision Metric function here'))\n",
    "    # Recall\n",
    "    print(\"Recall = {}\".format('Add Recall Metric function here'))\n",
    "    # FPR\n",
    "    print(\"FPR = {}\".format('Add FPR Metric function here'))\n",
    "    # TPR\n",
    "    print(\"TPR = {}\".format('Add TPR Metric function here'))\n",
    "  \n",
    "  \n",
    "print_performance_metrics(lrPredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn: Q6 - GBM - Evaluation\n",
    "\n",
    "### Now use the `print_performance_metrics()` function to evaluate the performance of the GBM model and compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_performance_metrics(gbPredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "\n",
    "For each model you can run the below comand to see its params and a brief explanation of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gb.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - Param Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "lrParamGrid = (ParamGridBuilder()\n",
    "               .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n",
    "               .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "               .addGrid(lr.maxIter, [2, 5])\n",
    "               .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Models (with all hyperparameter combinations):', len(lrParamGrid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn: Q7: GBM - Param Grid\n",
    "Build out a param grid for the gb model, call it 'gbParamGrid'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - Perform Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up an evaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "\n",
    "# Create CrossValidator\n",
    "lrCv = CrossValidator(estimator=lr, estimatorParamMaps=lrParamGrid, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "# Run cross validations\n",
    "lrCvModel = lrCv.fit(trainingData)\n",
    "# this will likely take a fair amount of time because of the amount of models that we're creating and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at best params from the CV\n",
    "print(lrCvModel.bestModel._java_obj.getRegParam())\n",
    "print(lrCvModel.bestModel._java_obj.getElasticNetParam())\n",
    "print(lrCvModel.bestModel._java_obj.getMaxIter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrCvModel.bestModel._java_obj.extractParamMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn: Q8: GBM - Perform Cross Validation\n",
    "- Perform cross validation of params on your 'gb' model.\n",
    "- Print out the best params you found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create CrossValidator\n",
    "\n",
    "# Run cross validations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at best params from the CV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - CV Model Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use test set to measure the accuracy of our model on new data\n",
    "lrCvPredictions = lrCvModel.transform(testData)\n",
    "\n",
    "lrCvPredictions.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBM - CV Model Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbCvPredictions = gbCvModel.transform(testData)\n",
    "\n",
    "gbCvPredictions.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - CV Model Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_performance_metrics(lrCvPredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBM - CV Model Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_performance_metrics(gbCvPredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - Model Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model Intercept: ', lrCvModel.bestModel.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrWeights = lrCvModel.bestModel.coefficients\n",
    "lrWeights = [(float(w),) for w in lrWeights]  # convert numpy type to float, and to tuple\n",
    "lrWeightsDF = sqlContext.createDataFrame(lrWeights, [\"Feature Weight\"])\n",
    "lrWeightsDF.toPandas().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Feature Importance\n",
    "\n",
    "Interpreting machine learning models is always a thing of paramount importance with business stakeholders always wanting to know how machine learning models really work. Feature importances can be an interesting way to tackle this problem. Let's explore how to find feature importances for our models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn: Q9: Print out a table of feature_name and feature_coefficient from the Logistic Regression model \n",
    "\n",
    "- Hint: Adapt the code from [here](https://stackoverflow.com/questions/42935914/how-to-map-features-from-the-output-of-a-vectorassembler-back-to-the-column-name) if needed as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn: Q10: Build and train a RandomForestClassifier and print out a table of feature importances from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
